# ğŸš¨ K8s Pod Monitoring & Alerting with OpenTelemetry

## ğŸ“Œ Problem Statement
Monitor Kubernetes pods and send an alert if a pod is stuck in `Pending` state for over **5 minutes** using **OpenTelemetry Collector**. Metrics should be visualized in **SigNoz** and alerts delivered via **Slack**.

---

## âœ… Prerequisites

- âœ… [Minikube](https://minikube.sigs.k8s.io/docs/start/)
- âœ… [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)
- âœ… [kubectx & kubens](https://github.com/ahmetb/kubectx)
- âœ… Slack webhook URL
- âœ… SigNoz account + access token

---

## ğŸ§° Tools Used
| Tool | Use |
|------|-----|
| Minikube | Local K8s cluster |
| OpenTelemetry Collector | Pod phase metric observer |
| SigNoz Cloud | Metric ingestion & alerting |
| Slack Webhook | Alert delivery |

---

## ğŸ“ Folder Structure
```bash
signoz-k8s-alerting/
â”œâ”€â”€ deployments/
â”‚   â””â”€â”€ nginx-deployments.yaml
â”œâ”€â”€ otel/
â”‚   â”œâ”€â”€ otel-rbac.yaml
â”‚   â”œâ”€â”€ otel-collector-config.yaml
â”‚   â””â”€â”€ otel-collector-deployment.yaml
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ pending-pod.png
â”‚   â”œâ”€â”€ alert-query.png
â”‚   â””â”€â”€ slack-alert.png
â”‚   â””â”€â”€ setup.png
â””â”€â”€ README.md
```

---

## ğŸ§­ High-Level Flow

1. Start Minikube and create required namespaces (`signoz`, `opentelemetry`).
2. Deploy two nginx pods - one runs normally, the other is stuck in `Pending` via `nodeSelector` mismatch.
3. Deploy OpenTelemetry Collector configured with:
   - `k8s_observer` extension: Discovers Kubernetes objects like pods.
   - `k8s_cluster` receiver: Exposes core Kubernetes metrics like `k8s.pod.phase`, which is required for alerting.
4. Send metrics to SigNoz using the `otlp` exporter.
5. In SigNoz, set up an alert rule for `k8s.pod.phase == Pending` persisting for 5+ minutes.
6. Send alert to a Slack channel via webhook.

---

## ğŸ› ï¸ Key Commands

```bash
# Start minikube
minikube start

# Create and switch to namespace
kubectl create ns signoz
kubens signoz

# Apply nginx deployments
kubectl apply -f deployments/nginx-deployments.yaml

# Create opentelemetry namespace
kubectl create ns opentelemetry

# Deploy OTEL Collector RBAC and config
kubectl apply -f otel/otel-rbac.yaml
kubectl apply -f otel/otel-collector-config.yaml
kubectl apply -f otel/otel-collector-deployment.yaml
```

---

## ğŸ“Š Visualize Metrics in SigNoz

1. Open [Sample Dashboard](https://svey-5nsw.in.signoz.cloud/dashboard/01983dbc-cb7a-7895-b491-92d32eb449b2?relativeTime=30m)
2. Create dashboard or use Explorer
3. Query `k8s.pod.phase` filtered by `Pending`

---

## ğŸ”” Alerting Steps

1. Go to **Alerts > Create Alert**
2. Metric: `k8s.pod.phase`
3. Condition: `Pending count > 0 for 5 minutes`
4. Slack integration: Add webhook URL

---

## ğŸ–¼ï¸ Screenshots

- ğŸ“¸ `screenshots/pending-pod.png`: Pod stuck in `Pending`
- ğŸ“¸ `screenshots/signoz-dashboard.png`: SigNoz showing metric
- ğŸ“¸ `screenshots/slack-alert.png`: Slack message after 5 minutes

---

## âœ… Expected Outcome

- Metrics are correctly ingested into SigNoz
- Alert fires after 5 minutes if pod remains pending
- Slack receives the alert message

---

## ğŸ§‘â€ğŸ’» Author Notes

This demo showcases how to integrate **Kubernetes**, **OpenTelemetry**, and **SigNoz** to detect real-time infrastructure issues and alert on them effectively.

---

## ğŸ“ Resources

- [OpenTelemetry K8s Observer Extension](https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/extension/observer/k8sobserver/README.md)
- [SigNoz Cloud](https://signoz.io)
- [Slack Incoming Webhooks](https://api.slack.com/messaging/webhooks)
